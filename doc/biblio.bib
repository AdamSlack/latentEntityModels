% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@inproceedings{rehurek_lrec,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}

@misc{Macnamee-pd,
author = {Mcnamee, Paul and T Dang, Hoa},
year = 2009,
month = 01,
title = {Overview of the TAC 2009 knowledge base population track}
}


@INPROCEEDINGS{Huang2009-xb,
  title     = "Part-of-speech tagger based on maximum entropy model",
  booktitle = "2009 2nd {IEEE} International Conference on Computer Science and
               Information Technology",
  author    = "Huang, H and Zhang, X",
  abstract  = "The maximum entropy (ME) conditional models don't force to
               adhere to the independence assumption such as in Hidden Markov
               generative models, and thus the ME-based part-of-speech (POS)
               tagger can depend on arbitrary, non-independent features, which
               are benefit to the POS tagging, without accounting for the
               distribution of those dependencies. Since ME models are able to
               flexibly utilize a wide variety of features, the sparse problem
               of training data is efficiently solved. Experiments show that
               the POS tagging error rate is reduced by 54.25\% in close test
               and 40.56\% in open test over the hidden-markov-Model-based
               baseline, and synchronously an accuracy of 98.01\% in close test
               and 95.56\%in open test are obtained.",
  pages     = "26--29",
  month     =  aug,
  year      =  2009,
  keywords  = "entropy;hidden Markov models;natural language processing;hidden
               Markov generative model;maximum entropy model;natural language
               processing;part-of-speech tagger;Automatic speech
               recognition;Computer science;Entropy;Hidden Markov
               models;Labeling;Natural language processing;Probability
               distribution;Smoothing methods;Tagging;Testing;Hidden Markov
               model (HMM);ME model;Natural Language Processing (NLP);POS
               tagging"
}


@INPROCEEDINGS{Newman2010-op,
  title     = "Automatic Evaluation of Topic Coherence",
  booktitle = "Human Language Technologies: The 2010 Annual Conference of the
               North American Chapter of the Association for Computational
               Linguistics",
  author    = "Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin,
               Timothy",
  publisher = "Association for Computational Linguistics",
  pages     = "100--108",
  series    = "HLT '10",
  year      =  2010,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Morstatter2016-co,
  title     = "A novel measure for coherence in statistical topic models",
  booktitle = "Proceedings of the 54th Annual Meeting of the Association for
               Computational Linguistics (Volume 2: Short Papers)",
  author    = "Morstatter, Fred and Liu, Huan",
  volume    =  2,
  pages     = "543--548",
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Aletras2013-oo,
  title     = "Evaluating topic coherence using distributional semantics",
  booktitle = "Proceedings of the 10th International Conference on
               Computational Semantics ({IWCS} 2013)--Long Papers",
  author    = "Aletras, Nikolaos and Stevenson, Mark",
  publisher = "aclweb.org",
  pages     = "13--22",
  year      =  2013
}

@INPROCEEDINGS{Crossno2011-sn,
  title     = "{TopicView}: Visually Comparing Topic Models of Text Collections",
  booktitle = "2011 {IEEE} 23rd International Conference on Tools with
               Artificial Intelligence",
  author    = "Crossno, P J and Wilson, A T and Shead, T M and Dunlavy, D M",
  abstract  = "We present Topic View, an application for visually comparing and
               exploring multiple models of text corpora. Topic View uses
               multiple linked views to visually analyze both the conceptual
               content and the document relationships in models generated using
               different algorithms. To illustrate Topic View, we apply it to
               models created using two standard approaches: Latent Semantic
               Analysis (LSA) and Latent Dirichlet Allocation (LDA). Conceptual
               content is compared through the combination of (i) a bipartite
               graph matching LSA concepts with LDA topics based on the cosine
               similarities of model factors and (ii) a table containing the
               terms for each LSA concept and LDA topic listed in decreasing
               order of importance. Document relationships are examined through
               the combination of (i) side-by-side document similarity graphs,
               (ii) a table listing the weights for each document's
               contribution to each concept/topic, and (iii) a full text reader
               for documents selected in either of the graphs or the table. We
               demonstrate the utility of Topic View's visual approach to model
               assessment by comparing LSA and LDA models of two example
               corpora.",
  pages     = "936--943",
  month     =  nov,
  year      =  2011,
  keywords  = "content management;data visualisation;graph theory;text
               analysis;LDA topics;TopicView;bipartite graph matching LSA
               concepts;conceptual content analysis;cosine
               similarities;document relationships;latent Dirichlet
               allocation;latent semantic analysis;model assessment;model
               factors;multiple linked views;side-by-side document similarity
               graphs;text collection;text corpora;text reader;topic
               model;visual model analysis;Analytical models;Bipartite
               graph;Computational modeling;Data
               models;Layout;Vectors;Visualization;latent dirichlet
               allocation;latent semantic analysis;text analysis;visual model
               analysis"
}


@INPROCEEDINGS{Huang1996-hu,
  title     = "Deleted interpolation and density sharing for continuous hidden
               Markov models",
  booktitle = "1996 {IEEE} International Conference on Acoustics, Speech, and
               Signal Processing Conference Proceedings",
  author    = "Huang, X D and Hwang, Mei-Yuh and Jiang, Li and Mahajan, M",
  abstract  = "As one of the most powerful smoothing techniques, deleted
               interpolation has been widely used in both discrete and
               semi-continuous hidden Markov model (HMM) based speech
               recognition systems. For continuous HMMs, most smoothing
               techniques are carried out on the parameters themselves such as
               Gaussian mean or covariance parameters. HMMs this paper, we
               propose to smooth the probability density values instead of the
               parameters of continuous HMMs. This allows us to use most of the
               existing smoothing techniques for both discrete and continuous
               HMMs. We also point out that our deleted interpolation can be
               regarded as a parameter sharing technique. We further generalize
               this sharing to the probability density function (PDF) level, in
               which each PDF becomes a basic unit and can be freely shared
               across any Markov state. For a wide range of dictation
               experiments, deleted interpolation reduced the word error
               rate-by 11\% to 23\% over other simple parameter smoothing
               techniques like flooring. Generic PDF sharing further reduced
               the error rate by 3\%",
  volume    =  2,
  pages     = "885--888 vol. 2",
  month     =  may,
  year      =  1996,
  keywords  = "Gaussian processes;covariance analysis;hidden Markov
               models;interpolation;probability;smoothing methods;speech
               recognition;Gaussian mean;HMM;Markov state;PDF;continuous hidden
               Markov models;covariance parameters;deleted
               interpolation;density sharing;dictation experiments;discrete
               hidden Markov model;parameter sharing technique;parameter
               smoothing;probability density;probability density
               function;semicontinuous hidden Markov model;smoothing
               techniques;word error rate reduction;Context modeling;Error
               analysis;Hidden Markov models;Interpolation;Power
               smoothing;Power system modeling;Probability
               distribution;Smoothing methods;Speech recognition;Training data"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ratnaparkhi1996-oa,
  title     = "A maximum entropy model for part-of-speech tagging",
  author    = "Ratnaparkhi, A",
  abstract  = "Abstract This paper presents a statistical model which trains
               from a corpus annotated with Part-Of-Speech tags and assigns
               them to previously unseen text with state-of-the-art accuracy
               (96.6\%). The model can be classified as a Maximum Entropy model
               and",
  journal   = "of the conference on empirical methods in …",
  publisher = "anthology.aclweb.org",
  year      =  1996
}

@INPROCEEDINGS{Cano_Basave2014-mx,
  title           = "Automatic labelling of topic models learned from Twitter
                     by summarisation",
  booktitle       = "The 52nd Annual Meeting of the Association for
                     Computational Linguistics: Proceedings of the Conference:
                     Volume 2: Short Papers",
  author          = "Cano Basave, Amparo Elizabeth and He, Yulan and Xu,
                     Ruifeng",
  abstract        = "Latent topics derived by topic models such as Latent
                     Dirichlet Allocation (LDA) are the result of hidden
                     thematic structures which provide further insights into
                     the data. The automatic labelling of such topics derived
                     from social media poses however new challenges since
                     topics may characterise novel events happening in the real
                     world. Existing automatic topic labelling approaches which
                     depend on external knowledge sources become less
                     applicable here since relevant articles/concepts of the
                     extracted topics may not exist in external sources. In
                     this paper we propose to address the problem of automatic
                     labelling of latent topics learned from Twitter as a
                     summarisation problem. We introduce a framework which
                     apply summarisation algorithms to generate topic labels.
                     These algorithms are independent of external sources and
                     only rely on the identification of dominant terms in
                     documents related to the latent topic. We compare the
                     efficiency of existing state of the art summarisation
                     algorithms. Our results suggest that summarisation
                     algorithms generate better topic labels which capture
                     event-related context compared to the top-n terms returned
                     by LDA.",
  publisher       = "Association for Computational Linguistics (ACL)",
  pages           = "618--624",
  month           =  jun,
  year            =  2014,
  keywords        = "topic models, automatic labelling",
  conference      = "52nd Annual Meeting of the Association for Computational
                     Linguistics"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Biro2008-ld,
  title     = "Latent Dirichlet Allocation in Web Spam Filtering",
  booktitle = "Proceedings of the 4th International Workshop on Adversarial
               Information Retrieval on the Web",
  author    = "B{\'\i}r{\'o}, Istv{\'a}n and Szab{\'o}, J{\'a}cint and
               Bencz{\'u}r, Andr{\'a}s A",
  abstract  = "… For classification we used the machine learning toolkit Weka
               [23] … 2. METHOD First we describe latent Dirichlet allocation
               [4]. For a de- tailed elaboration, we refer to Heinrich [13] …
               As a simplest solution we may classify d as spam if its LDA
               prediction is above a certain threshold …",
  publisher = "ACM",
  pages     = "29--32",
  series    = "AIRWeb '08",
  year      =  2008,
  address   = "New York, NY, USA",
  keywords  = "document classification, feature selection, information
               retrieval, latent dirichlet allocation, text analysis, web
               content spam"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Borthwick1999-tg,
  title     = "A maximum entropy approach to named entity recognition",
  author    = "Borthwick, A and Grishman, R",
  publisher = "cs.nyu.edu",
  year      =  1999
}

@ARTICLE{Kumar2004-da,
  title    = "On the Performance of Latent Semantic Indexing-based Information
              Retrieval",
  author   = "Kumar, Cherukuri Aswani and Srinivas, Suripeddi",
  abstract = "Conventional vector based Information Retrieval (IR) models,
              Vector Space Model (VSM) and Generalized Vector Space Model
              (GVSM), represents documents and queries as vectors in a
              multidimensional space. This high dimensional data places great
              demands for computing resources. To overcome these problems,
              Latent Semantic Indexing (LSI): a variant of VSM, projects the
              documents into a lower dimensional space, computed via Singular
              Value Decomposition. It is stated in IR literature that LSI model
              is 30\% more effective than classical VSM models. However
              statistical significance tests are required to evaluate the
              reliability of such comparisons. But to the best of our knowledge
              significance of performance of LSI model is not analyzed so far.
              Focus of this paper is to address this issue. We discuss the
              tradeoffs of VSM, GVSM and LSI and empirically evaluate the
              difference in performance on four testing document collections.
              Then we analyze the statistical significance of these performance
              differences.",
  journal  = "CIT. Journal of Computing and Information Technology",
  volume   =  17,
  number   =  3,
  pages    = "259--264",
  month    =  oct,
  year     =  2004
}


@INPROCEEDINGS{Han2012-gy,
  title     = "An Entity-topic Model for Entity Linking",
  booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in
               Natural Language Processing and Computational Natural Language
               Learning",
  author    = "Han, Xianpei and Sun, Le",
  publisher = "Association for Computational Linguistics",
  pages     = "105--115",
  series    = "EMNLP-CoNLL '12",
  year      =  2012,
  address   = "Stroudsburg, PA, USA"
}


@ARTICLE{Chib1995-wq,
  title     = "Marginal Likelihood from the Gibbs Output",
  author    = "Chib, Siddhartha",
  abstract  = "In the context of Bayes estimation via Gibbs sampling, with or
               without data augmentation, a simple approach is developed for
               computing the marginal density of the sample data (marginal
               likelihood) given parameter draws from the posterior
               distribution. Consequently, Bayes factors for model comparisons
               can be routinely computed as a by-product of the simulation.
               Hitherto, this calculation has proved extremely challenging. Our
               approach exploits the fact that the marginal density can be
               expressed as the prior times the likelihood function over the
               posterior density. This simple identity holds for any parameter
               value. An estimate of the posterior density is shown to be
               available if all complete conditional densities used in the
               Gibbs sampler have closed-form expressions. To improve accuracy,
               the posterior density is estimated at a high density point, and
               the numerical standard error of resulting estimate is derived.
               The ideas are applied to probit regression and finite mixture
               models.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  90,
  number    =  432,
  pages     = "1313--1321",
  year      =  1995
}

@PHDTHESIS{Wallach2008-ti,
  title  = "Structured topic models for language",
  author = "Wallach, Hanna Megan",
  year   =  2008,
  school = "University of Cambridge"
}

@INPROCEEDINGS{Wallach2009-ot,
  title     = "Evaluation Methods for Topic Models",
  booktitle = "Proceedings of the 26th Annual International Conference on
               Machine Learning",
  author    = "Wallach, Hanna M and Murray, Iain and Salakhutdinov, Ruslan and
               Mimno, David",
  publisher = "ACM",
  pages     = "1105--1112",
  series    = "ICML '09",
  year      =  2009,
  address   = "New York, NY, USA"
}

@ARTICLE{Newton1994-ws,
  title     = "Approximate Bayesian Inference with the Weighted Likelihood
               Bootstrap",
  author    = "Newton, Michael A and Raftery, Adrian E",
  abstract  = "We introduce the weighted likelihood bootstrap (WLB) as a way to
               simulate approximately from a posterior distribution. This
               method is often easy to implement, requiring only an algorithm
               for calculating the maximum likelihood estimator, such as
               iteratively reweighted least squares. In the generic weighting
               scheme, the WLB is first order correct under quite general
               conditions. Inaccuracies can be removed by using the WLB as a
               source of samples in the sampling-importance resampling (SIR)
               algorithm, which also allows incorporation of particular prior
               information. The SIR-adjusted WLB can be a competitive
               alternative to other integration methods in certain models.
               Asymptotic expansions elucidate the second-order properties of
               the WLB, which is a generalization of Rubin's Bayesian
               bootstrap. The calculation of approximate Bayes factors for
               model comparison is also considered. We note that, given a
               sample simulated from the posterior distribution, the required
               marginal likelihood may be simulation consistently estimated by
               the harmonic mean of the associated likelihood values; a
               modification of this estimator that avoids instability is also
               noted. These methods provide simple ways of calculating
               approximate Bayes factors and posterior model probabilities for
               a very wide class of models.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  56,
  number    =  1,
  pages     = "3--48",
  year      =  1994
}

@ARTICLE{Blei2003-dj,
  title    = "Latent Dirichlet Allocation",
  author   = "Blei, David M and Ng, Andrew Y and Jordan, Michael I",
  journal  = "J. Mach. Learn. Res.",
  volume   =  3,
  number   = "Jan",
  pages    = "993--1022",
  year     =  2003
}

@INPROCEEDINGS{Hofmann1999-qb,
  title     = "Probabilistic Latent Semantic Indexing",
  booktitle = "Proceedings of the 22Nd Annual International {ACM} {SIGIR}
               Conference on Research and Development in Information Retrieval",
  author    = "Hofmann, Thomas",
  publisher = "ACM",
  pages     = "50--57",
  series    = "SIGIR '99",
  year      =  1999,
  address   = "New York, NY, USA"
}

@INPROCEEDINGS{Bender2003-lc,
  title     = "Maximum Entropy Models for Named Entity Recognition",
  booktitle = "Proceedings of the Seventh Conference on Natural Language
               Learning at {HLT-NAACL} 2003 - Volume 4",
  author    = "Bender, Oliver and Och, Franz Josef and Ney, Hermann",
  abstract  = "Abstract In this paper, we describe a system that applies
               maximum entropy (ME) models to the task of named entity
               recognition (NER). Starting with an annotated corpus and a set
               of features which are easily obtainable for almost any language,
               we first build a baseline NE",
  publisher = "Association for Computational Linguistics",
  pages     = "148--151",
  series    = "CONLL '03",
  year      =  2003,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Lafferty2001-ab,
  title     = "Conditional Random Fields: Probabilistic Models for Segmenting
               and Labeling Sequence Data",
  booktitle = "Proceedings of the Eighteenth International Conference on
               Machine Learning",
  author    = "Lafferty, John D and McCallum, Andrew and Pereira, Fernando C N",
  publisher = "Morgan Kaufmann Publishers Inc.",
  pages     = "282--289",
  month     =  jun,
  year      =  2001
}

@INPROCEEDINGS{Zhou2002-st,
  title     = "Named Entity Recognition Using an {HMM-based} Chunk Tagger",
  booktitle = "Proceedings of the 40th Annual Meeting on Association for
               Computational Linguistics",
  author    = "Zhou, Guodong and Su, Jian",
  publisher = "Association for Computational Linguistics",
  pages     = "473--480",
  series    = "ACL '02",
  year      =  2002,
  address   = "Stroudsburg, PA, USA"
}

@ARTICLE{Malecha2010-fl,
  title     = "Maximum entropy part-of-speech tagging in {NLTK}",
  author    = "Malecha, G and Smith, I",
  abstract  = "Abstract In this paper we implement a part of speech tagger for
               NLTK using maximum entropy methods. Our tagger can be used as a
               drop-in replacement for any of the other NLTK taggers. We give a
               brief tutorial on how to use our tagger as well as describing
               the implementation at a high level. We evaluate our tagger on
               the Penn Tree Bank and compare our results to those of previous
               work.",
  journal   = "unpublished course-related report",
  publisher = "pdfs.semanticscholar.org",
  year      =  2010
}

@INPROCEEDINGS{Camelin2011-ub,
  title     = "Unsupervised Concept Annotation Using Latent Dirichlet
               Allocation and Segmental Methods",
  booktitle = "Proceedings of the First Workshop on Unsupervised Learning in
               {NLP}",
  author    = "Camelin, Nathalie and Detienne, Boris and Huet, St{\'e}phane and
               Quadri, Dominique and Lef{\`e}vre, Fabrice",
  publisher = "Association for Computational Linguistics",
  pages     = "72--81",
  series    = "EMNLP '11",
  year      =  2011,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Xie2015-wv,
  title     = "Incorporating word correlation knowledge into topic modeling",
  booktitle = "Proceedings of the 2015 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  author    = "Xie, Pengtao and Yang, Diyi and Xing, Eric",
  pages     = "725--734",
  year      =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brill1995-sr,
  title     = "Transformation-based Error-driven Learning and Natural Language
               Processing: A Case Study in Part-of-speech Tagging",
  author    = "Brill, Eric",
  abstract  = "… automated part - of - speech tagging was initially explored
               (Klein and Sim- mons 1963; Harris 1962), people manually
               engineered rules for tagging … append best rule to ordered list
               of transformations … To remedy this problem, we extend the
               transformation- based tagger by adding …",
  journal   = "Comput. Linguist.",
  publisher = "MIT Press",
  volume    =  21,
  number    =  4,
  pages     = "543--565",
  month     =  dec,
  year      =  1995,
  address   = "Cambridge, MA, USA"
}

@MISC{noauthor_undated-ik,
  title        = "The Stanford Natural Language Processing Group",
  howpublished = "\url{https://nlp.stanford.edu/projects/project-ner.shtml}",
  note         = "Accessed: 2018-1-10"
}

@ARTICLE{Nadeau2007-tp,
  title     = "A survey of named entity recognition and classification",
  author    = "Nadeau, David and Sekine, Satoshi",
  abstract  = "This survey covers fifteen years of research in the Named Entity
               Recognition and Classification (NERC) field, from 1991 to 2006.
               We report observations about languages, named entity types,
               domains and textual genres studied in the literature. From the
               start, NERC systems have been developed using hand-made rules,
               but now machine learning techniques are widely used. These
               techniques are surveyed along with other critical aspects of
               NERC such as features and evaluation methods. Features are
               word-level, dictionary-level and corpus-level representations of
               words in a document. Evaluation techniques, ranging from
               intuitive exact match to very complex matching techniques with
               adjustable cost of errors, are an indisputable key to progress.",
  journal   = "Lingvistic{\ae} Investigationes",
  publisher = "John Benjamins",
  volume    =  30,
  number    =  1,
  pages     = "3--26",
  month     =  jan,
  year      =  2007
}

@INPROCEEDINGS{Wallach2006-fm,
  title     = "Topic Modeling: Beyond Bag-of-words",
  booktitle = "Proceedings of the 23rd International Conference on Machine
               Learning",
  author    = "Wallach, Hanna M",
  publisher = "ACM",
  pages     = "977--984",
  series    = "ICML '06",
  year      =  2006,
  address   = "New York, NY, USA"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Ratinov2009-gw,
  title     = "Design Challenges and Misconceptions in Named Entity Recognition",
  booktitle = "Proceedings of the Thirteenth Conference on Computational
               Natural Language Learning",
  author    = "Ratinov, Lev and Roth, Dan",
  abstract  = "Abstract We analyze some of the fundamental design challenges
               and misconceptions that underlie the development of an efficient
               and robust NER system. In particular, we address issues such as
               the representation of text chunks, the inference approach needed
               to combine local NER decisions, the sources of prior knowledge
               and how to use them within an NER system. In the process of
               comparing several solutions to these challenges we reach some …",
  publisher = "Association for Computational Linguistics",
  pages     = "147--155",
  series    = "CoNLL '09",
  year      =  2009,
  address   = "Stroudsburg, PA, USA"
}

@ARTICLE{Tjong_Kim2003-ym,
  title         = "Introduction to the {CoNLL-2003} Shared Task:
                   {Language-Independent} Named Entity Recognition",
  author        = "Tjong Kim, Erik and De Meulder, Fien",
  abstract      = "We describe the CoNLL-2003 shared task: language-independent
                   named entity recognition. We give background information on
                   the data sets (English and German) and the evaluation
                   method, present a general overview of the systems that have
                   taken part in the task and discuss their performance.",
  month         =  jun,
  year          =  2003,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "cs/0306050"
}
 @INPROCEEDINGS{Leech1994-rh,
   title     = "{CLAWS4}: The Tagging of the British National Corpus",
   booktitle = "Proceedings of the 15th Conference on Computational Linguistics
                - Volume 1",
   author    = "Leech, Geoffrey and Garside, Roger and Bryant, Michael",
   publisher = "Association for Computational Linguistics",
   pages     = "622--628",
   series    = "COLING '94",
   year      =  1994,
   address   = "Stroudsburg, PA, USA"
 }

@ARTICLE{Atdag2013-qo,
  title         = "A Comparison of Named Entity Recognition Tools Applied to
                   Biographical Texts",
  author        = "Atda{\u g}, Samet and Labatut, Vincent",
  abstract      = "Named entity recognition (NER) is a popular domain of
                   natural language processing. For this reason, many tools
                   exist to perform this task. Amongst other points, they
                   differ in the processing method they rely upon, the entity
                   types they can detect, the nature of the text they can
                   handle, and their input/output formats. This makes it
                   difficult for a user to select an appropriate NER tool for a
                   specific situation. In this article, we try to answer this
                   question in the context of biographic texts. For this
                   matter, we first constitute a new corpus by annotating
                   Wikipedia articles. We then select publicly available, well
                   known and free for research NER tools for comparison:
                   Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i
                   LingPipe. We apply them to our corpus, assess their
                   performances and compare them. When considering overall
                   performances, a clear hierarchy emerges: Stanford has the
                   best results, followed by LingPipe, Illionois and
                   OpenCalais. However, a more detailed evaluation performed
                   relatively to entity types and article categories highlights
                   the fact their performances are diversely influenced by
                   those factors. This complementarity opens an interesting
                   perspective regarding the combination of these individual
                   tools in order to improve performance.",
  month         =  aug,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1308.0661"
}

@PATENT{Deerwester1989-yl,
  title    = "Computer information retrieval using latent semantic structure",
  author   = "Deerwester, Scott C and Dumais, Susan T and Furnas, George W and
              Harshman, Richard A and Landauer, Thomas K and Lochbaum, Karen E
              and Streeter, Lynn A",
  abstract = "A methodology for retrieving textual data objects is disclosed.
              The information is treated in the statistical domain by presuming
              that there is an underlying, latent semantic structure in the
              usage of words in the data objects. Estimates to this latent
              structure are utilized to represent and retrieve objects. A user
              query is recouched in the new statistical domain and then
              processed in the computer system to extract the underlying
              meaning to respond to the query.",
  number   =  4839853,
  month    =  jun,
  year     =  1989
}

@INPROCEEDINGS{Cutting1992-vx,
  title     = "A Practical Part-of-speech Tagger",
  booktitle = "Proceedings of the Third Conference on Applied Natural Language
               Processing",
  author    = "Cutting, Doug and Kupiec, Julian and Pedersen, Jan and Sibun,
               Penelope",
  publisher = "Association for Computational Linguistics",
  pages     = "133--140",
  series    = "ANLC '92",
  year      =  1992,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Feng2010-dp,
  title     = "Topic Models for Image Annotation and Text Illustration",
  booktitle = "Human Language Technologies: The 2010 Annual Conference of the
               North American Chapter of the Association for Computational
               Linguistics",
  author    = "Feng, Yansong and Lapata, Mirella",
  publisher = "Association for Computational Linguistics",
  pages     = "831--839",
  series    = "HLT '10",
  year      =  2010,
  address   = "Stroudsburg, PA, USA"
}

@ARTICLE{Finkel2005-uz,
  title    = "Incorporating Non-local Information into Information Extraction
              Systems by Gibbs Sampling",
  author   = "Finkel, J R and Grenager, T and Manning, C",
  journal  = "Proceedings of the 43nd Annual Meeting of the Association for
              Computational Linguistics (ACL 2005)",
  pages    = "363--370",
  year     =  2005
}

@ARTICLE{Landauer1998-kx,
  title     = "An introduction to latent semantic analysis",
  author    = "Landauer, Thomas K and Foltz, Peter W and Laham, Darrell",
  abstract  = "Latent Semantic Analysis (LSA) is a theory and method for
               extracting and representing the contextual?usage meaning of
               words by statistical computations applied to a large corpus of
               text (Landauer it mimics human word sorting and category
               judgments; it simulates word?word and passage?word lexical
               priming data; and, as reported in 3 following articles in this
               issue, it accurately estimates passage coherence, learnability
               of passages by individual students, and the quality and quantity
               of knowledge contained in an essay.",
  journal   = "Discourse Process.",
  publisher = "Routledge",
  volume    =  25,
  number    = "2-3",
  pages     = "259--284",
  month     =  jan,
  year      =  1998
}

@ARTICLE{Jurafsky2014-yb,
  title     = "Speech and language processing",
  author    = "Jurafsky, D and Martin, J H",
  abstract  = "The idea of giving computers the ability to process human
               language is as old as the idea of computers themselves. This
               book is about the implementation and implications of that
               exciting idea. We introduce a vibrant interdisciplinary field
               with many names corresponding",
  publisher = "cs.colorado.edu",
  year      =  2014
}

@ARTICLE{Zhang2011-dn,
  title   = "Automatic Image Annotation and Retrieval Using the Latent
             Dirichlet Allocation Model",
  author  = "Zhang, Tong and Lu, Zhe-Ming and Chan, Kap Luk and Li, Zhen",
  journal = "IJCSES International Journal of Computer Sciences and Engineering
             Systems",
  volume  =  5,
  number  =  1,
  year    =  2011
}

@INCOLLECTION{Chang2009-jr,
  title     = "Reading Tea Leaves: How Humans Interpret Topic Models",
  booktitle = "Advances in Neural Information Processing Systems 22",
  author    = "Chang, Jonathan and Gerrish, Sean and Wang, Chong and
               Boyd-graber, Jordan L and Blei, David M",
  editor    = "Bengio, Y and Schuurmans, D and Lafferty, J D and Williams, C K
               I and Culotta, A",
  publisher = "Curran Associates, Inc.",
  pages     = "288--296",
  year      =  2009
}

@INPROCEEDINGS{McCallum2003-yu,
  title     = "Early Results for Named Entity Recognition with Conditional
               Random Fields, Feature Induction and Web-enhanced Lexicons",
  booktitle = "Proceedings of the Seventh Conference on Natural Language
               Learning at {HLT-NAACL} 2003 - Volume 4",
  author    = "McCallum, Andrew and Li, Wei",
  abstract  = "Abstract Models for many natural language tasks benefit from the
               flexibility to use overlapping, non-independent features. For
               example, the need for labeled data can be drastically reduced by
               taking advantage of domain knowledge in the form of word lists,
               part-",
  publisher = "Association for Computational Linguistics",
  pages     = "188--191",
  series    = "CONLL '03",
  year      =  2003,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Lau2011-pu,
  title     = "Automatic Labelling of Topic Models",
  booktitle = "Proceedings of the 49th Annual Meeting of the Association for
               Computational Linguistics: Human Language Technologies - Volume
               1",
  author    = "Lau, Jey Han and Grieser, Karl and Newman, David and Baldwin,
               Timothy",
  publisher = "Association for Computational Linguistics",
  pages     = "1536--1545",
  series    = "HLT '11",
  year      =  2011,
  address   = "Stroudsburg, PA, USA"
}

@INPROCEEDINGS{Brill1992-hh,
  title     = "A Simple Rule-based Part of Speech Tagger",
  booktitle = "Proceedings of the Workshop on Speech and Natural Language",
  author    = "Brill, Eric",
  abstract  = "Abstract Automatic part of speech tagging is an area of natural
               language processing where statistical techniques have been more
               successful than rule - based methods. In this paper, we present
               a simple rule - based part of speech tagger which automatically
               acquires its rules and",
  publisher = "Association for Computational Linguistics",
  pages     = "112--116",
  series    = "HLT '91",
  year      =  1992,
  address   = "Stroudsburg, PA, USA"
}

@MISC{WebServer-lc,
  title        = "Web Server Survey | Netcraft",
  howpublished = "\url{https://news.netcraft.com/archives/category/web-server-survey/}",
  note         = "Accessed: 2017-10-16"
}
